# ðŸ¤– AI Code Assistant

## Overview
Intelligent coding assistant powered by Groq's llama-3.3-70b model, providing real-time code completion, explanations, and AI-driven development support - completely FREE.

---

## âœ¨ Key Features

### 1. **Inline Code Completion**
- AI-powered suggestions as you type
- Context-aware completions
- Ghost text preview
- 800ms debounce for smooth experience

### 2. **AI Chat Assistant**
- Interactive coding help
- Ask questions in natural language
- Get explanations and solutions
- Code generation from descriptions

### 3. **Code Explanation**
- Select any code snippet
- Right-click â†’ "Explain Code"
- Get detailed AI explanations
- Understand complex logic

### 4. **Code Generation**
- Write comments describing functionality
- AI generates complete code
- Support for all languages
- Automatic integration

### 5. **Refactoring Suggestions**
- AI-driven code improvements
- Performance optimization tips
- Best practices recommendations
- Clean code suggestions

---

## ðŸŽ¯ Usage

### Inline Completion
```javascript
// Start typing...
function calculateSum(

// Wait 800ms - AI suggests:
function calculateSum(arr) {
  return arr.reduce((sum, num) => sum + num, 0);
}

// Press Tab to accept
```

### Chat Assistant
```javascript
// Click AI icon or Ctrl+Shift+A
Chat: "How do I read a CSV file in Python?"

AI Response:
"To read a CSV file in Python, use the pandas library:

import pandas as pd
df = pd.read_csv('file.csv')
print(df.head())
"
```

### Code Explanation
```python
# Select this code
def factorial(n):
    return 1 if n <= 1 else n * factorial(n-1)

# Right-click â†’ "Explain Code"

AI: "This is a recursive function that calculates 
factorial. Base case: nâ‰¤1 returns 1. Otherwise, 
multiplies n by factorial of (n-1)."
```

### Code Generation
```javascript
// Write comment
// TODO: Create a function to validate email addresses

// Lightbulb appears â†’ Click

// AI generates:
function validateEmail(email) {
  const regex = /^[^\s@]+@[^\s@]+\.[^\s@]+$/;
  return regex.test(email);
}
```

---

## ðŸ”§ Technical Details

### Provider Configuration
- **AI Provider**: Groq (FREE tier)
- **Model**: llama-3.3-70b-versatile
- **API**: https://api.groq.com/openai/v1
- **Speed**: ~500 tokens/second
- **Cost**: 100% FREE

### Components
```
frontend/src/components/AIAssistant.jsx       # Chat UI
frontend/src/services/aiService.js            # API client
frontend/src/services/aiCompletionProvider.js # Monaco integration
backend/src/services/ai.service.js            # Groq integration
backend/src/controllers/ai.controller.js      # API endpoints
```

### API Endpoints
```javascript
POST /api/v1/ai/chat               # Chat with AI
POST /api/v1/ai/completion         # Inline completion
POST /api/v1/ai/explain            # Explain code
POST /api/v1/ai/generate           # Generate code
POST /api/v1/ai/refactor           # Refactoring suggestions
GET  /api/v1/ai/status             # Check availability
```

---

## ðŸš€ Features in Detail

### 1. Inline Completion
```javascript
// Configuration
{
  debounceDelay: 800,  // ms before triggering
  maxTokens: 200,      // max completion length
  temperature: 0.2,    // low = more deterministic
  provider: 'groq'     // AI provider
}

// Trigger conditions
- Typing stopped for 800ms
- Cursor at end of line
- Not in string/comment
- Editor has focus
```

### 2. Chat Interface
```javascript
// Message format
{
  role: 'user',
  content: 'Your question here'
}

// Response
{
  success: true,
  response: 'AI answer',
  provider: 'groq',
  model: 'llama-3.3-70b-versatile'
}
```

### 3. Context-Aware Assistance
```javascript
// AI receives:
- Current file content
- Language type
- Cursor position
- Selected code (if any)
- File path

// Provides:
- Relevant completions
- Language-specific help
- Project context awareness
```

---

## âš™ï¸ Configuration

### Setup Groq API Key
```bash
# Get free API key from https://console.groq.com

# Add to backend/.env
GROQ_API_KEY=gsk_your_api_key_here
```

### Adjust Settings
```javascript
// backend/src/services/ai.service.js

// Completion settings
async getCodeCompletion({
  maxTokens: 200,      // Increase for longer completions
  temperature: 0.2,    // 0.0-1.0 (higher = more creative)
  top_p: 0.95         // Nucleus sampling
}) {
  // ...
}

// Chat settings
async chat({
  maxTokens: 1000,     // Longer responses
  temperature: 0.7     // More conversational
}) {
  // ...
}
```

### Customize Debounce Delay
```javascript
// frontend/src/components/EnhancedMonacoEditor.jsx
const DEBOUNCE_DELAY = 800; // Change to 500ms for faster suggestions
```

---

## ðŸŽ¨ AI Capabilities

### Supported Tasks
âœ… Code completion
âœ… Bug explanation
âœ… Algorithm implementation
âœ… Code review
âœ… Performance optimization
âœ… Documentation generation
âœ… Test case creation
âœ… Design pattern suggestions
âœ… Security vulnerability detection
âœ… Code translation (language to language)

### Example Prompts
```javascript
// Good prompts:
"Explain this bubble sort implementation"
"Refactor this code to use async/await"
"Generate unit tests for this function"
"What's wrong with this code?"
"Convert this loop to a list comprehension"

// Avoid vague prompts:
"Fix this" (provide context)
"Make it better" (specify what to improve)
```

---

## ðŸ“Š Performance Metrics

| Metric | Value | Notes |
|--------|-------|-------|
| **Response Time** | <2s | Groq is very fast |
| **Completion Accuracy** | ~85% | Context-dependent |
| **Uptime** | 99.9% | Groq infrastructure |
| **Rate Limit** | High | Free tier generous |
| **Token Limit** | 8K context | Model limitation |

---

## ðŸ› Troubleshooting

### Issue: AI not responding
**Solution**:
```bash
# Check API key
cat backend/.env | grep GROQ_API_KEY

# Check backend logs
cd backend
npm run dev
# Look for: "âœ… AI Service initialized with: groq (FREE tier)"

# Test API key
curl https://api.groq.com/openai/v1/chat/completions \
  -H "Authorization: Bearer $GROQ_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"model":"llama-3.3-70b-versatile","messages":[{"role":"user","content":"Hi"}]}'
```

### Issue: Inline completion not appearing
**Solution**:
- Wait full 800ms after typing stops
- Ensure cursor is at end of line
- Check browser console for errors
- Verify Monaco model is not disposed

### Issue: Slow AI responses
**Solution**:
- Groq is usually fast (<2s)
- Check internet connection
- Reduce maxTokens for faster responses
- API may have temporary slowdown

---

## ðŸ”’ Privacy & Security

### Data Handling
- **No storage**: Code not stored by Groq
- **Ephemeral**: Requests deleted after response
- **Encrypted**: HTTPS for all API calls
- **No training**: Your code not used for AI training

### Best Practices
- Don't send sensitive credentials in code
- Review AI suggestions before accepting
- Test generated code thoroughly
- Keep API keys secure in .env file

---

## ðŸ“– References

- [Groq Documentation](https://console.groq.com/docs)
- [llama-3.3 Model Card](https://huggingface.co/meta-llama/Llama-3.3-70B)
- [Monaco Editor InlineCompletionsProvider](https://microsoft.github.io/monaco-editor/api/interfaces/monaco.languages.InlineCompletionsProvider.html)
